--
title: "Running Time of Matrix Multiplication"
author: "Bøe, Lehre, Rønold"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(FrF2)
library(BsMD)
library(MASS)
library(nortest)
library(knitr)
set.seed(42)
```
## 1. Issue to be addressed
This report presents a two-level factorial experiment in which we consider how different factors affect the running time of a program performing computations on a laptop. As students of industrial mathematics, many of our courses rely heavily on information technology, and a significant amount of time is spent on programming. It would be useful to know more about which factors have the most influence on the performance of our programs. 

There already exists benchmark tests and a lot of literature evaluating the effects of different factors on the performance of computer programs. These often evaluate general performance, and therefore take into consideration different tasks that are not relevant throughout our courses. We've therefore chosen to perform a speed test on matrix multiplication, an operation that is relevant for all our courses. 

## 2. Selection of factors and levels
We have chosen four factors that we believe are relevant for the running time of the matrix multiplication. The first factor is which programming language the computation is executed in. We have chosen the languages Python and R which are the primary languages used for our courses. 

The second factor considered is the computer hardware specifications. We excpect the CPU clockrate to be the most significant part of this factor, but as two computers only differing in processor frequency were not available for the experiment, the hardware specifications are considered as a factor ranging from "High" to "Low", since e.g. both CPU and RAM are better on the superior computer.

The third factor is whether vectorized functions are used to perform the calculations. A common mistake while learning scientific computing is using nested ```for```-loops and indexing of arrays to perform computations. Most languages have already implemented and optimized  vectorized functions, performing these operations on all array elements simultanously. We want to investigate the differences between these vectorized functions and the *naive* approach using nested ```for```-loops.

The fourth factor is memory usage, i.e whether or not there are other programs running on the computer, hogging RAM and other resources. We believe that other programs running on the PC will slow down the matrix multiplication. 

As this is a two-level factorial experiment, all factor levels are recoded to -1 or 1. For language and vectorization of functions, which are qualitative factors, this recoding is quite natural. Either we write in Python or R, and calculations are performed using vectorized or non-vectorized functions. We've considered hardware specifications as a binary variable. Through a different design of the experiment it's possible to quantify this variable, e.g. by CPU clockrate, which we may have used for predictions on processors not included in this experiment. Similarily we've only considered whether we run programs in the background or not, but it could have been possible to take into consideration the amount of resources required for these programs to run. 

Since we are working with computers it is relatively easy to ensure that the factors are at their desired level. The most challenging factor is the effect of hardware specifications for which we've performed the experiments on two different computers. To eliminate effects of the impact different software configurations may have on computations, we used a Macbook Air and a Macbook Pro, both updated to the newest version of OS X, and the latest releases of the programming languages. To ensure that the computational cost of the background programs was the same for all the experiments we ran the same programs on both computers.

## 3. Selection of response variable
An obvious choice of response variable for this problem is the time spent to perform the calculations, and using seconds as unit of measurement makes most sense for the scale of our experiment. Time measurement is easily performed to a high degree of precision by using basic functions in the different programming languages: Let the program prepare all necessary data and variables, log the internal time for the computer before and after the calculations, and print their difference. The variability of starting and stopping the watch is of order $10^{-7}$, and is therefore considered negligible.

## 4. Choice of design
The experiment was designed as a full $2^4$ factorial design, which resulted in a total of 16 runs. As the complete experiment could be carried out in a short amount of time, it was unnecessary to use a fractional factorial design.

This is an experiment in which we have a high degree of control over the factors. Matrices of equal size were used throughout the experiment, and the computers had the same initial settings for each run (that being that the computers were not running any unwanted background processes). While it is true that we do not have complete control over how the computers spend their processing power, the variations should be very small and we therefore deemed it unnecessary to perform replicates of the experiment. Blocking of the runs is also unnecessary for this experiment, since there is no reason to divide the experiments into parts. All of the runs can be performed successively.

## 5. Implementation of the experiment
Before the experiments were performed, both computers were rebooted. All programs and background tasks listed in the task manager, that were deemed **not** necessary for normal performance, were terminated. Two programs were written in both R and Python, one for vectorized computations, and one using the naive approach, calculating the matrix product entries element-wise using for loops and indexing. All four programs construct two random matrices of size $200\times200$ using the same probability distribution. Then the start time is stored in a variable, before the same matrix multiplication is performed 30 times using a ```for```-loop. Immediately on exit from the loop, the end time is recorded, and the difference between end and start time is printed to the screen. To measure the effect of increased memory usage, two instances of a python script performing matrix calculations in an endlesss loop until interrupted, were running for all the relevant experiments. 

On both computers, we performed a fresh install of Anaconda Python 3.6 and R 3.4.3. All scripts were run from terminal, and thus we have minimal impact from IDE's (integrated development environment) on the performance. 

The order of the tests were performed according to the plan given by the ```FrF2()``` function in ```R``` which gives a randomized test setup. The labels are given by

```{r table}#, echo = FALSE}
factors = c("Language(A)", "Hardware(B)", "Vectorized(C)", "RAM Usage(D)")
factorlabs = c("A", "B", "C", "D")
table <-matrix(c("R", "Python", "High", "Low", "Vectorized", "Naive", "High", "Low"), 2, 4, dimnames = list(c(1, -1), factors))
kable(table)
```

Each experiment was done independently. Between each run we quit the programs that had been running on the computers. One might argue that we should have restarted the computers before each experiment, but we believed that this would not have any significant impact on the result of the experiments.

```{r design}#, include = FALSE}
plan <- FrF2(nruns=2^4, nfactors=4, randomize=TRUE, factornames = factornames)
plan
```

## 6. Analysis of data

```{r fit_model, echo = F}
#Response variable
y <- scan("response.txt")
plan <- add.response(plan, y)
# Fit model
fit4 <- lm(y~.^4, data = plan)
summary(fit4)$coef
#fit4$coef
```
The summary shows the estimated coefficients for the regression model. We see that all of the residuals are 0 since we have 16 explanatory variables and 16 data points. Furthermore, it appears that A (language) and C (vectorization) contribute the most to the running time of the code. The interaction between these is also important for the time. As the levels of the factors we have chosen for this experiment are not assosciated with any continous numerical values, the intercept does not have a clear interpretation here. 




```{r, echo = F}
# Visualization
MEPlot(fit4) # main effects plot
IAPlot(fit4) # interaction effect plots
```

```{r, include = F}
effects <- 2*fit4$coeff
effects
```

The main effects plot is a visualization of the summary above. It shows the impact on the response that stems from changing the level of each factor.
For all pairs of variables, where one is changed and the other fixed, the interaction plot matrix shows the corresponding change in the response. Thus parallell lines indicate that there is little interaction between the given pair of variables and vice versa. In accordance with what we saw in the summary, it appears that the strongest interaction is between factors A and C. Additionally, there seems to be some interaction between factors C and D, although it may not be significant.

```{r, echo = F}
# Displays cut-off values for significant variables according to Lenths. May remove plot if not deemed necessary.
LenthPlot(fit4, plt = T)
```

In the Lenth plot we see the effect values for each of the factors and the interactions. The dotted lines are for either the ME (margin of error) or SME (simultaneous margin of error) cutoff. These values are calculated from the PSE (Pseudo Standard Error), which is an estimate of the standard deviation for each of the effects. Let $\beta_j$ is the $j$'th regression coefficient, and thus $2\beta_j$ is the effect related to the corresponding factor. Furthermore, let $m$ be the number of factors. Then under the null hypothesis that an effect is equal to zero the statistic $T = 2\beta_j / PSE$ is approximately t distributed with $m/3$ degrees of freedom. Then an effect is considered significant if $|2\beta_j| > t_{\alpha/2, m/3} \cdot PSE$. Here $t_{\alpha/2, m/3}$ is the ME value. There is an $\alpha$% chance that the effect value is greater than the ME value under the null hypothesis. Similarly one can compute the SME value such that there is an $\alpha$% chance that any of the effects are greater than the SME. Thus if an effect is greater than the SME it should be considered highly significant.

Through inspection of the Lenth plot, vectorizing of functions is clearly an active effect, as its value is greater than the SME. The value for language choice seems significant as as well, since its effect value is higher than the ME. This is not a surprising result, but what is more interesting is the interaction value between language and vectorization. Even though we saw a large increase for vectorization in R, the difference was small compared to the difference in Python, where the non-vectorized functions were almost as high as a factor thousand times the running time for the vectorized function. Through further investigation, we found an explanation for this. The ```numpy``` library which is used for scientific computing in Python, is largely written in C and therefore avoids a lot of the time consuming operations performed "behind the scenes" in standard Python.

Another way to estimate the variance, is to make the assumption that high order interactions, with three or more factors are zero. Under this assumption, the regression model fit is given by
```{r more_info, echo = F}
fit2 <- lm(y~.^2,data=plan)
summary(fit2)
# 1. fitted vs studentized residuals
rres <- rstudent(fit2)
plot(fit2$fitted,rres)
# 2. normality of residuals
qqnorm(rres)
qqline(rres)
ad.test(rstudent(fit2))
```
From the Q-Q plot it's not easy to conclude anything. There is a trend for the lower quantiles in which the residuals lie below the true line, as well as an anomaly for the rightmost datapoint. The rightmost point corresponds to the experiment with an exeptionally high response of 220 seconds. A plausible cause for this would be an increase in the computers temperature, slowing down the computer during the experiment. For the leftmost datapoints it's hard to find any similarities that would explain the low values. Considering the p-value from the Anderson-Darling test we may not reject a normal distribution of the residuals either. From the residual plot it appears that there is some trend in the residuals, so the assumption that they are identically distributed may not be correct. However, we have quite few data points and therefore it is difficult to conclude with anything. 

## 7. Conclusion and recommendations
In conclusion the most significant effects among those tested for the running time of matrix multiplication are vectorization of functions, and choice of language. The interaction between these plays an important role as well, as the difference between vectorized and non-vectorized functions affects the running time to a larger degree for Python than for R. 

In comparison to these effects, memory usage and hardware differences do not have a signifcant effect on running time. Further experiments should be performed using only vectorized functions, to investigate how these effects affect the program running time. It could also be interesting to find out if the relations are similar for other calculations such as solving linear systems, or for sparse matrix computations.

\newpage
# Appendix A: Program Code
```{r, eval = F}
# R NAIVE CODE
A = matrix(rnorm(40000, mean=0, sd=1), nrow = 200, ncol=200)
B = matrix(rnorm(40000, mean=0, sd=1), nrow = 200, ncol=200)
manual = function(A,B){
  C = matrix(0L, nrow = dim(A)[1], ncol = dim(A)[2])
  for (i in 1:dim(A)[1]){
    
    for (j in 1: dim(A)[1]){
      sum = 0
      for (k in 1:dim(A)[1]){
        sum = sum + A[i,k]*B[k,j]
      }
      C[i,j] = sum
    }
  }
    
  return (C)
}
indicator = "Ferdig"
start = Sys.time()
for (i in 1:30){
  manual(A,B)
}
end = Sys.time()
print(end-start)
```

```{r, eval = F}
# R VECTORIZED CODE
set.seed(39)
A = matrix(rnorm(40000, 0, 1), nrow = 200, ncol=200)
B = matrix(rnorm(40000, 0, 1), nrow = 200, ncol=200)
start = Sys.time()
for (i in 1:30) {
  C = A%*%B
}
end = Sys.time()
print(end- start)
```