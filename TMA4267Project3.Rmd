---
title: "Volume loss  "
>>>>>>> aa3de34790fc227f36c08c4ae1e98f0ed321fb94
author: "Bøe, Lehre, Rønold"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(FrF2)
library(BsMD)
library(MASS)
library(nortest)
library(knitr)
set.seed(42)
```
## 1. Issue to be addressed
This report presents a two-level factorial experiment in which we consider how different factors affect the running time of a program performing computations on a laptop. As students of industrial mathematics, many of our courses rely heavily on information technology, and a significant amount of time is spent on programming. It would be useful to know more about which factors have the most influence on the performance of our programs. 

There already exists benchmark tests and a lot of literature evaluating the effects of different factors on the performance of computer programs. These often evaluate general performance, and therefore take into consideration different tasks that are not relevant throughout our courses. We've therefore chosen to perform a speed test on matrix multiplication, an operation that is relevant for all our courses. 

## 2. Selection of factors and levels
We have chosen four factors that we believe are relevant for the running time of the matrix multiplication. The first factor is which programming language the computation is executed in. We have chosen the languages Python and R which are the primary languages used for our courses. 

The second factor considered is the computer hardware specifications. We excpect the CPU clockrate to be the most significant part of this factor, but as two computers only differing in processor frequency were not available for the experiment, the hardware specifications are considered as a factor ranging from "High" to "Low", since e.g. both CPU and RAM are better on the superior computer.

The third factor is whether vectorized functions are used to perform the calculations. A common mistake while learning scientific computing is using nested ```for```-loops and indexing of arrays to perform computations. Most languages have already implemented and optimized  vectorized functions, performing these operations on all array elements simultanously. We want to investigate the differences between these vectorized functions and the *naive* approach using nested ```for```-loops.

The fourth factor is memory usage, i.e whether or not there are other programs running on the computer, hogging RAM and other resources. We believe that other programs running on the PC will slow down the matrix multiplication. 

As this is a two-level factorial experiment, all factor levels are recoded to -1 or 1. For language and vectorization of functions, which are qualitative factors, this recoding is quite natural. Either we write in Python or R, and calculations are performed using vectorized or non-vectorized functions. We've considered hardware specifications as a binary variable. Through a different design of the experiment it's possible to quantify this variable, e.g. by CPU clockrate, which we may have used for predictions on processors not included in this experiment. Similarily we've only considered whether we run programs in the background or not, but it could have been possible to take into consideration the amount of resources required for these programs to run. 

Since we are working with computers it is relatively easy to ensure that the factors are at their desired level. The most challenging factor is the effect of hardware specifications for which we've performed the experiments on two different computers. To eliminate effects of the impact different software configurations may have on computations, we used a Macbook Air and a Macbook Pro, both updated to the newest version of OS X, and the latest releases of the programming languages. To ensure that the computational cost of the background programs was the same for all the experiments we ran the same programs on both computers.

## 3. Selection of response variable
An obvious choice of response variable for this problem is the time spent to perform the calculations, and using seconds as unit of measurement makes most sense for the scale of our experiment. Time measurement is easily performed to a high degree of precision by using basic functions in the different programming languages: Let the program prepare all necessary data and variables, log the internal time for the computer before and after the calculations, and print their difference. The variability of starting and stopping the watch is of order $10^{-7}$, and is therefore considered negligible.

## 4. Choice of design
The experiment was designed as a $2^3$ factorial design including one replication, which resulted in a total of 16 runs. As the complete experiment could be carried out in a short amount of time, it was unnecessary to use a fractional factorial design.

This is an experiment in which we have a high degree of control over the factors. At the same time, the measurements had some uncertanty to it as we did not have the best equipment. This fact in addition to few factors made it clear that we needed to have replication of the experiments to get enough measurements. Blocking on the other hand was deemed unnecessary for this experiment, since there was no reason to divide the experiments into parts. All of the runs were repeated under the same conditions.

## 5. Implementation of the experiment
To perform the experiment, some testing had to be performed on the equipment. First, we had to find the optimal way of calculating the mass reduction. Our first though was to have a scale under the bucket to measure the weight, but the scales we had available were chosen to be to uncertain. Instead, we obtained a bucket that was sylindrical. This made it easy to measure difference in volume as we could just measure the difference in height before and after the drop and then do some volume-estimations as the cross-section on each level of the bucket would be identical. To make sure that the drop was the same every time, we built a platform to put the bottle on with a string attached to it. To run the experiment, we would just pull the string such that the bottle could fall. The other two factors, size and weight, was very easy to keep constant as the size only was dependent on the bottle we used and the weight was kept equal since the bottle was sealed. Lastly, before every drop, we filled up the bucket to the edge to make sure that the volume of water in the bucket before the drop was the same every run.

The order of the tests were performed according to the plan given by the ```FrF2()``` function in ```R``` which gives a randomized test setup. The labels are given by

```{r table}#, echo = FALSE}
factors = c("Hoyde(H)", "Innhold(I)", "Str(S)")
factorlabs = c("H", "I", "S")
table <-matrix(c("Hoyt", "Lavt", "Hel", "Halv", "Stor", "Liten"), 2, 3, dimnames = list(c(1, -1), factors))
kable(table)
```

Each experiment was done independently. Between each run, we reset the platform to its original state and filled the bucket to the same level as before the last drop.

```{r design}#, include = TRUE}
plan <- FrF2(nruns=2^3, nfactors=3, randomize=FALSE,replications = 2)

```

## 6. Analysis of data

```{r fit_model, echo = F, eval=FALSE}
#Response variable
y <- scan("response.txt")
plan <- add.response(plan, y)
# Fit model
plan
fit4 <- lm(y~(A+B+C)^3, data = plan)
summary(fit4)$coef
#fit4$coef
```
The summary shows the estimated coefficients for the regression model. We see that all of the residuals are 0 since we have 16 explanatory variables and 16 data points. Furthermore, it appears that A (language) and C (vectorization) contribute the most to the running time of the code. The interaction between these is also important for the time. As the levels of the factors we have chosen for this experiment are not assosciated with any continous numerical values, the intercept does not have a clear interpretation here. 




```{r, echo = F, eval = FALSE}
# Visualization

anova(fit4)
MEPlot(fit4) # main effects plot
IAPlot(fit4) # interaction effect plots
```

```{r, include = F, eval = FALSE}
effects <- 2*fit4$coeff
effects
```

The main effects plot is a visualization of the summary above. It shows the impact on the response that stems from changing the level of each factor.
For all pairs of variables, where one is changed and the other fixed, the interaction plot matrix shows the corresponding change in the response. Thus parallell lines indicate that there is little interaction between the given pair of variables and vice versa. In accordance with what we saw in the summary, it appears that the strongest interaction is between factors A and C. Additionally, there seems to be some interaction between factors C and D, although it may not be significant.

```{r, echo = F, eval = FALSE}
# Displays cut-off values for significant variables according to Lenths. May remove plot if not deemed necessary.
LenthPlot(fit4, plt = T)
```

In the Lenth plot we see the effect values for each of the factors and the interactions. The dotted lines are for either the ME (margin of error) or SME (simultaneous margin of error) cutoff. These values are calculated from the PSE (Pseudo Standard Error), which is an estimate of the standard deviation for each of the effects. Let $\beta_j$ is the $j$'th regression coefficient, and thus $2\beta_j$ is the effect related to the corresponding factor. Furthermore, let $m$ be the number of factors. Then under the null hypothesis that an effect is equal to zero the statistic $T = 2\beta_j / PSE$ is approximately t distributed with $m/3$ degrees of freedom. Then an effect is considered significant if $|2\beta_j| > t_{\alpha/2, m/3} \cdot PSE$. Here $t_{\alpha/2, m/3}$ is the ME value. There is an $\alpha$% chance that the effect value is greater than the ME value under the null hypothesis. Similarly one can compute the SME value such that there is an $\alpha$% chance that any of the effects are greater than the SME. Thus if an effect is greater than the SME it should be considered highly significant.

Through inspection of the Lenth plot, vectorizing of functions is clearly an active effect, as its value is greater than the SME. The value for language choice seems significant as as well, since its effect value is higher than the ME. This is not a surprising result, but what is more interesting is the interaction value between language and vectorization. Even though we saw a large increase for vectorization in R, the difference was small compared to the difference in Python, where the non-vectorized functions were almost as high as a factor thousand times the running time for the vectorized function. Through further investigation, we found an explanation for this. The ```numpy``` library which is used for scientific computing in Python, is largely written in C and therefore avoids a lot of the time consuming operations performed "behind the scenes" in standard Python.

Another way to estimate the variance, is to make the assumption that high order interactions, with three or more factors are zero. Under this assumption, the regression model fit is given by
```{r more_info, echo = F, eval = FALSE}
fit2 <- lm(y~.^2,data=plan)
summary(fit2)
# 1. fitted vs studentized residuals
rres <- rstudent(fit2)
plot(fit2$fitted,rres)
# 2. normality of residuals
qqnorm(rres)
qqline(rres)
ad.test(rstudent(fit2))
```
From the Q-Q plot it's not easy to conclude anything. There is a trend for the lower quantiles in which the residuals lie below the true line, as well as an anomaly for the rightmost datapoint. The rightmost point corresponds to the experiment with an exeptionally high response of 220 seconds. A plausible cause for this would be an increase in the computers temperature, slowing down the computer during the experiment. For the leftmost datapoints it's hard to find any similarities that would explain the low values. Considering the p-value from the Anderson-Darling test we may not reject a normal distribution of the residuals either. From the residual plot it appears that there is some trend in the residuals, so the assumption that they are identically distributed may not be correct. However, we have quite few data points and therefore it is difficult to conclude with anything. 

## 7. Conclusion and recommendations
In conclusion the most significant effects among those tested for the running time of matrix multiplication are vectorization of functions, and choice of language. The interaction between these plays an important role as well, as the difference between vectorized and non-vectorized functions affects the running time to a larger degree for Python than for R. 

In comparison to these effects, memory usage and hardware differences do not have a signifcant effect on running time. Further experiments should be performed using only vectorized functions, to investigate how these effects affect the program running time. It could also be interesting to find out if the relations are similar for other calculations such as solving linear systems, or for sparse matrix computations.
